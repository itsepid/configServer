input {
    jdbc {
        jdbc_driver_library => "/usr/share/logstash/sqljdbc_12.4/enu/jars/mssql-jdbc-12.4.2.jre8.jar"
        jdbc_driver_class => "com.microsoft.sqlserver.jdbc.SQLServerDriver"
        jdbc_connection_string => "jdbc:sqlserver://94.182.95.169:9125;user=sa;password=Qwer@#1234;databaseName=KhabarAzMa;encrypt=true;trustServerCertificate=true"
        jdbc_user => "sa"
        jdbc_password => "Qwer@#1234"
        jdbc_paging_mode => "auto"
        jdbc_paging_enabled => true
        jdbc_page_size => 5000
        jdbc_fetch_size => 500
        last_run_metadata_path => "/usr/share/logstash/.logstash_jdbc_last_run"
        tracking_column => "id"
        use_column_value => true
        tracking_column_type => "numeric"
        schedule => "* * * * *"
        statement => "SELECT TOP(50000) id, createdate, title, keywords, summary, source FROM KhabarAzMa.dbo.News WHERE id > :sql_last_value ORDER BY id ASC"
    }
}

filter {
  ruby {
    code => '
      # Read the current sql_last_value from the file
      sql_last_value = File.read("/usr/share/logstash/config/sql_last_value.txt")

      # Update the sql_last_value based on the last processed row
   sql_last_value = event.get("id")
      #sql_last_value = (sql_last_value.to_i + 100000).to_s

      # Write the updated sql_last_value back to the file
      File.write("/usr/share/logstash/config/sql_last_value.txt", sql_last_value)
    '
  }
}


#filter {
#  ruby {
#    path => "/usr/share/logstash/config/update_sql_last_value.rb"
#  }
#}

output {
   elasticsearch {
      hosts => "${ELASTICSEARCH_HOST}"
      user => "elastic"
      password => "${LOGSTASH_INTERNAL_PASSWORD}"
      ssl => true
      cacert => "config/ca.crt"
      index => "khabar-news"
      document_id => "%{id}"
   }
}
